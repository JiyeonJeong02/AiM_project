import os
import json
import re
import requests
from transformers import pipeline, AutoTokenizer
from dotenv import load_dotenv

# âœ… .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# âœ… í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
ELASTICSEARCH_URL = os.getenv("ELASTICSEARCH_URL")
KIBANA_URL = os.getenv("KIBANA_URL")
INDEX_NAME = os.getenv("INDEX_NAME", "business_overview")

# âœ… KoBART ëª¨ë¸ ì„¤ì •
model_name = "digit82/kobart-summarization"
tokenizer = AutoTokenizer.from_pretrained(model_name)
summarizer = pipeline("summarization", model=model_name, tokenizer=tokenizer)

# âœ… JSON íŒŒì¼ ê²½ë¡œ (ê¸°ì—… ë³´ê³ ì„œ ì›ë³¸ íŒŒì¼)
JSON_FILE_PATH = "output/company_reports.json"

def remove_financial_info(text):
    """ì¬ë¬´ ì •ë³´, ì‹ ìš©ë“±ê¸‰ ê´€ë ¨ ì •ë³´ ì œê±° (ì—°ë„/ë‚ ì§œëŠ” ìœ ì§€)"""
    text = re.sub(r"(?i)([A-Za-z]* ?Bonds? -? [0-9A-Za-z ]+|Issuer Credit Rating|ì±„ê¶Œë³´ë‹¤ëŠ”|ì›ë¦¬ê¸ˆ ì§€ê¸‰ëŠ¥ë ¥|ê¸°ì—…ì‹ ìš©í‰ê°€)", "", text)
    text = re.sub(r"(?i)(Moodys|S&P|Fitch|í•œêµ­ê¸°ì—…í‰ê°€|í•œêµ­ì‹ ìš©í‰ê°€)", "", text)  # ì‹ ìš©ë“±ê¸‰ ê¸°ê´€ ì œê±°
    text = re.sub(r"(?i)(AAA|AA|A3|BBB|BB|CCC|CC|C|D)([\s-]*ì•ˆì •ì |[\s-]*ë¶€ì •ì |[\s-]*ê¸ì •ì )?", "", text)  # ì‹ ìš©ë“±ê¸‰ ì œê±°
    text = re.sub(r"\(ì£¼\d+\)", "", text)  # "(ì£¼1)" ê°™ì€ ì£¼ì„ ì œê±°
    text = re.sub(r"\s+", " ", text).strip()  # ì—°ì†ëœ ê³µë°± ì œê±°
    return text

def remove_redundant_words(text):
    """ë°˜ë³µë˜ëŠ” ê¸°ì—…ëª… ì ‘ë¯¸ì–´ (Ltd., Inc., Co., LLC., ë“±) ì œê±°"""
    redundant_patterns = [
        r"\bLtd\.,?", r"\bInc\.,?", r"\bCo\.,?", r"\bLLC\.,?",
        r"\bGmbH\.,?", r"\bCorp\.,?", r"\bPLC\.,?", r"\bLimited\.,?"
    ]

    for pattern in redundant_patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì œê±°

    # âœ… ì—°ì†ëœ ì¤‘ë³µ ë‹¨ì–´ ì œê±° (ì˜ˆ: "Ltd. Ltd. Ltd." â†’ "Ltd.")
    text = re.sub(r"\b(\w+)( \1)+\b", r"\1", text)

    text = re.sub(r"\s+", " ", text).strip()  # âœ… ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    return text

def preprocess_text(text, company_name):
    """íšŒì‚¬ëª… í•œ ë²ˆ ìœ ì§€ + ê¸°ì—…ëª… ì ‘ë¯¸ì–´ ì œê±° + ì¬ë¬´ ì •ë³´ ì œê±°"""
    text = re.sub(r"\s+", " ", text)  # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r"[^\w\s.,!?]", "", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°

    # âœ… íšŒì‚¬ëª…ì´ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µë  ê²½ìš°, ì²« ë²ˆì§¸ ë“±ì¥ ì´í›„ ì‚­ì œ
    text = re.sub(fr"({company_name})\b.*?\b\1", r"\1", text)

    # âœ… ê¸°ì—…ëª… ì ‘ë¯¸ì–´ ì œê±° (Ltd., Inc., ë“±)
    text = remove_redundant_words(text)

    # âœ… ë¶ˆí•„ìš”í•œ ì¬ë¬´ ì •ë³´ ì œê±° (ì—°ë„/ë‚ ì§œëŠ” ìœ ì§€)
    text = remove_financial_info(text)

    return text.strip()

def remove_duplicate_sentences(text):
    """ìš”ì•½ëœ ë¬¸ì¥ì—ì„œ ì¤‘ë³µëœ ë¬¸ì¥ì„ ì œê±°"""
    sentences = text.split(". ")
    seen = set()
    filtered_sentences = []

    for sentence in sentences:
        if sentence not in seen:
            filtered_sentences.append(sentence)
            seen.add(sentence)

    return ". ".join(filtered_sentences)

def summarize_text(text_chunks, company_name):
    """ê°ê°ì˜ í…ìŠ¤íŠ¸ ì¡°ê°ì„ ìš”ì•½í•œ í›„, ìµœì¢… ìš”ì•½"""
    summaries = []
    
    for chunk in text_chunks:
        chunk = preprocess_text(chunk, company_name)  # âœ… ì „ì²˜ë¦¬ ì ìš©
        input_length = len(tokenizer.encode(chunk, add_special_tokens=False))
        max_length = min(700, max(100, input_length // 2))

        try:
            summary = summarizer(chunk, max_length=max_length, min_length=50, do_sample=False)
            result = summary[0]["summary_text"]

            # âœ… ë¹„ì •ìƒì ì¸ ìš”ì•½ ê°ì§€ í›„ ì¬ì²˜ë¦¬
            if len(set(result.split())) < 5:  
                print("âš  ë¹„ì •ìƒì ì¸ ìš”ì•½ ê°ì§€, ë‹¤ì‹œ ìš”ì•½ ì‹œë„...")
                result = summarizer(chunk, max_length=max(200, input_length // 3), min_length=50, do_sample=True)[0]["summary_text"]
            
            summaries.append(result)
        except Exception as e:
            print(f"âŒ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            summaries.append("ìš”ì•½ ì‹¤íŒ¨")

    # âœ… ì „ì²´ ìš”ì•½ ì¡°ê°ì„ ë‹¤ì‹œ í•˜ë‚˜ì˜ ìš”ì•½ìœ¼ë¡œ ì²˜ë¦¬
    combined_summary = " ".join(summaries)
    combined_summary = remove_duplicate_sentences(combined_summary)  # âœ… ì¤‘ë³µ ë¬¸ì¥ ì œê±°

    if len(tokenizer.encode(combined_summary, add_special_tokens=False)) > 1024:
        try:
            final_summary = summarizer(combined_summary, max_length=700, min_length=100, do_sample=False)
            return remove_duplicate_sentences(final_summary[0]["summary_text"])  # âœ… ìµœì¢… ìš”ì•½ì—ì„œë„ ì¤‘ë³µ ì œê±°
        except Exception as e:
            print(f"âŒ ìµœì¢… ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return combined_summary  # ìµœì¢… ìš”ì•½ ì‹¤íŒ¨ ì‹œ, ë¶€ë¶„ ìš”ì•½ ê²°ê³¼ ë°˜í™˜
    
    return combined_summary

def load_json_data(file_path):
    """ JSON íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    for company in data:
        company_name = company.get("company_name", "Unknown")
        business_overview = company.get("business_overview", "")

        if not business_overview:
            print(f"âš  {company_name}ì˜ ì‚¬ì—… ê°œìš” ì—†ìŒ, ê±´ë„ˆëœ€.")
            continue

        text_chunks = [business_overview]  
        summarized_text = summarize_text(text_chunks, company_name)

        company["business_overview_summary"] = summarized_text

    return data

def upload_to_elasticsearch(data):
    """Elasticsearchì— ë°ì´í„° ì—…ë¡œë“œ (ì—…ë°ì´íŠ¸ ë˜ëŠ” ì‚½ì…)"""
    headers = {"Content-Type": "application/json"}

    for company in data:
        company_name = company.get("company_name")
        business_overview_summary = company.get("business_overview_summary", "")

        if not business_overview_summary:
            print(f"âš  {company_name}ì˜ ìš”ì•½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
            continue

        update_url = f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_update_by_query"
        update_query = {
            "script": {
                "source": "ctx._source.business_overview_summary = params.summary",
                "params": {"summary": business_overview_summary}
            },
            "query": {
                "match": {
                    "company_name": company_name
                }
            }
        }
        response = requests.post(update_url, headers=headers, json=update_query)

        if response.status_code == 200:
            print(f"ğŸ”„ {company_name} ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        else:
            print(f"âŒ {company_name} ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨! {response.text}")

if __name__ == "__main__":
    json_data = load_json_data(JSON_FILE_PATH)
    upload_to_elasticsearch(json_data)
