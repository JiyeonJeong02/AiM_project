import requests
import zipfile
import os
import json
import time
from io import BytesIO
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import re
import xml.etree.ElementTree as ET

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Open DART API í‚¤ ê°€ì ¸ì˜¤ê¸°
API_KEY = os.getenv('DART_API_KEY')

# ì €ì¥í•  í´ë” ìƒì„±
save_dir = "downloads"
os.makedirs(save_dir, exist_ok=True)

# ğŸ”¹ `reprt_code` ìš°ì„ ìˆœìœ„ (1ë¶„ê¸° â†’ ë°˜ê¸° â†’ 3ë¶„ê¸° â†’ ì‚¬ì—…)
REPORT_CODES = ["11013", "11012", "11014", "11011"]

# ğŸ”¹ ì €ì¥í•  ë³´ê³ ì„œ ìœ í˜• (ê°ì‚¬ë³´ê³ ì„œ ì œì™¸)
VALID_REPORTS = ["ë°˜ê¸°ë³´ê³ ì„œ", "ë¶„ê¸°ë³´ê³ ì„œ", "ì‚¬ì—…ë³´ê³ ì„œ"]

# ğŸ”¹ KOSPI/KOSDAQ/KONEX ìƒì¥ê¸°ì—… í¬í•¨í•˜ì—¬ ì¡°íšŒ
def get_large_corp_codes():
    url = f"https://opendart.fss.or.kr/api/corpCode.xml?crtfc_key={API_KEY}"
    response = requests.get(url)

    if response.status_code == 200:
        with zipfile.ZipFile(BytesIO(response.content)) as zfile:
            zfile.extractall(save_dir)
        print("âœ… ê¸°ì—… ì½”ë“œ ZIP ë‹¤ìš´ë¡œë“œ ë° ì¶”ì¶œ ì™„ë£Œ.")

        xml_path = os.path.join(save_dir, "corpCode.xml")
        tree = ET.parse(xml_path)
        root = tree.getroot()

        corp_codes = {}
        for corp in root.findall("list"):
            corp_name = corp.find("corp_name").text.strip()
            corp_code = corp.find("corp_code").text.strip()
            stock_code = corp.find("stock_code").text.strip()  # ìƒì¥ëœ ì¢…ëª© ì½”ë“œ

            # ğŸ”´ KOSPI, KOSDAQ, KONEX ìƒì¥ê¸°ì—…ë§Œ í¬í•¨
            if stock_code and stock_code.startswith(("0", "1", "2", "3")):
                corp_codes[corp_name] = corp_code

        return corp_codes
    else:
        print("âŒ ê¸°ì—… ì½”ë“œ XML ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        return {}

# ğŸ”¹ íŠ¹ì • ê¸°ì—…ì˜ ìµœì‹  ë³´ê³ ì„œ ì ‘ìˆ˜ë²ˆí˜¸(rcept_no) ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±° + ê°ì‚¬ë³´ê³ ì„œ ì œì™¸)
def get_latest_rcept_nos(corp_code):
    valid_reports = set()
    for reprt_code in REPORT_CODES:
        url = "https://opendart.fss.or.kr/api/list.json"
        params = {
            'crtfc_key': API_KEY,
            'corp_code': corp_code,
            'bgn_de': '20240101',
            'end_de': '20241231',
            'reprt_code': reprt_code,
            'page_no': 1,
            'page_count': 10
        }
        
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            if 'list' in data and len(data['list']) > 0:
                for report in data['list']:
                    report_nm = report['report_nm']

                    # ğŸ”´ ê°ì‚¬ë³´ê³ ì„œ ì œì™¸í•˜ê³  ë°˜ê¸°/ë¶„ê¸°/ì‚¬ì—…ë³´ê³ ì„œë§Œ ì €ì¥
                    if any(valid_name in report_nm for valid_name in VALID_REPORTS):
                        rcept_no = report['rcept_no']
                        if rcept_no not in valid_reports:
                            print(f"ğŸ“Œ {corp_code}: {reprt_code} ë³´ê³ ì„œ ì„ íƒë¨ ({report_nm})")
                            valid_reports.add(rcept_no)

    return list(valid_reports)

# ğŸ”¹ íŠ¹ì • ê¸°ì—…ì˜ "ì‚¬ì—… ê°œìš”" ì¶”ì¶œ
def extract_business_overview(corp_name, corp_code):
    rcept_nos = get_latest_rcept_nos(corp_code)
    if not rcept_nos:
        print(f"âš ï¸ {corp_name}({corp_code})ì˜ ë°˜ê¸°/ë¶„ê¸°/ì‚¬ì—…ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    extracted_contents = []
    for rcept_no in rcept_nos:
        url = "https://opendart.fss.or.kr/api/document.xml"
        params = {'crtfc_key': API_KEY, 'rcept_no': rcept_no}

        response = requests.get(url, params=params)
        if response.status_code == 200:
            with zipfile.ZipFile(BytesIO(response.content)) as z:
                file_list = z.namelist()
                extracted_file = file_list[0]
                xml_path = os.path.join(save_dir, extracted_file)

                with open(xml_path, "wb") as f:
                    f.write(z.read(extracted_file))

            print(f"âœ… {corp_name}({corp_code}) ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {xml_path}")
        else:
            continue  # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë³´ê³ ì„œë¡œ ì§„í–‰

        # XML íŒŒì¼ ë¡œë“œ
        with open(xml_path, "r", encoding="utf-8", errors="ignore") as f:
            xml_content = f.read()

        soup = BeautifulSoup(xml_content, features="xml")

        def extract_section(section_title):
            section = soup.find("TITLE", string=lambda text: text and re.search(rf"\d*\.*\s*{section_title}", text))
            if not section:
                return None

            content = []
            for sibling in section.find_all_next():
                if sibling.name == "TITLE":
                    break
                if sibling.name in ["P", "TABLE", "SPAN", "DIV"]:
                    text = sibling.get_text(strip=True)
                    if text and text not in content:
                        content.append(text)

            return "\n".join(content) if content else None

        business_overview = extract_section("ì‚¬ì—…ì˜ ê°œìš”")
        if business_overview:
            extracted_contents.append(business_overview)

    return "\n\n".join(extracted_contents) if extracted_contents else None

# ğŸ”¹ ìƒì¥ê¸°ì—… ëŒ€ìƒìœ¼ë¡œ ì‹¤í–‰ (ì¡°íšŒ ê¸°ì—… ìˆ˜ ì¦ê°€)
corp_codes = get_large_corp_codes()  # ğŸ”´ ìƒì¥ê¸°ì—… ìš°ì„  ì¡°íšŒ

valid_data = []
for corp_name, corp_code in list(corp_codes.items())[200:400]:  # ğŸ”´ ì¡°íšŒ ê¸°ì—… ìˆ˜ 200ê°œ
    print(f"\nğŸ“Œ {corp_name}({corp_code}) - ìµœì‹  ë³´ê³ ì„œ ì¡°íšŒ ì¤‘...")
    business_overview = extract_business_overview(corp_name, corp_code)

    if business_overview:
        valid_data.append({"ê¸°ì—…ëª…": corp_name, "ì‚¬ì—… ê°œìš”": business_overview})
    else:
        print(f"âš ï¸ {corp_name}ì˜ ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")

    time.sleep(1)  # API ìš”ì²­ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•´ 1ì´ˆ ëŒ€ê¸°

# 5. JSON íŒŒì¼ ì €ì¥
with open("ê¸°ì—…_ì‚¬ì—…ê°œìš”.json", "w", encoding="utf-8") as f:
    json.dump(valid_data, f, ensure_ascii=False, indent=4)

print(f"ğŸ“„ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: ê¸°ì—…_ì‚¬ì—…ê°œìš”.json")
