import requests
import json
import re
import os
import zipfile
import xml.etree.ElementTree as ET
from io import BytesIO
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
import gc

class BusinessAnalysisSystem:
    def __init__(self):
        """Initialize with API settings"""
        # Load environment variables
        load_dotenv()
        
        # DART API settings
        self.api_key = os.getenv('DART_API_KEY')
        if not self.api_key:
            raise ValueError("DART_API_KEY environment variable is not set")
            
        # OpenAI API settings
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
            
        self.base_url = "https://opendart.fss.or.kr/api"
        self.companies = {
            "ì‚¼ì„±ì „ì": "005930",
            "SKí•˜ì´ë‹‰ìŠ¤": "000660",
            "í˜„ëŒ€ìë™ì°¨": "005380",
            "NAVER": "035420",
            "ì¹´ì¹´ì˜¤": "035720"
        }
        self.corp_codes = {}
        
        # Elasticsearch settings
        self.es_url = os.getenv("ELASTICSEARCH_URL")
        self.index_name = os.getenv("INDEX_NAME", "business_overview")

    def download_corp_codes(self):
        """Download company unique codes"""
        url = f"{self.base_url}/corpCode.xml"
        params = {"crtfc_key": self.api_key}
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            with zipfile.ZipFile(BytesIO(response.content)) as zf:
                xml_data = zf.read('CORPCODE.xml')
                
            root = ET.fromstring(xml_data)
            for company in root.findall('.//list'):
                corp_code = company.findtext('corp_code')
                stock_code = company.findtext('stock_code')
                if stock_code and stock_code.strip():
                    self.corp_codes[stock_code] = corp_code
            
            print("Corporate code list downloaded successfully")
            
        except Exception as e:
            print(f"Error downloading corporate codes: {e}")
            raise

    def get_company_info(self, company_name, stock_code):
        """Get basic company information"""
        url = f"{self.base_url}/company.json"
        params = {
            "crtfc_key": self.api_key,
            "stock_code": stock_code
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if data.get("status") == "000":
                return {
                    "ê¸°ì—…ëª…": data.get("corp_name"),
                    "ì˜ë¬¸ëª…": data.get("corp_name_eng"),
                    "ì¢…ëª©ì½”ë“œ": data.get("stock_code"),
                    "ëŒ€í‘œìëª…": data.get("ceo_nm"),
                    "ë²•ì¸êµ¬ë¶„": data.get("corp_cls"),
                    "ì„¤ë¦½ì¼": data.get("est_dt"),
                    "ìƒì¥ì¼": data.get("listing_dt"),
                    "ì—…ì¢…": data.get("induty_code"),
                    "í™ˆí˜ì´ì§€": data.get("hm_url"),
                    "ì£¼ì†Œ": data.get("adres")
                }
            return data
            
        except requests.exceptions.RequestException as e:
            print(f"Error retrieving company info ({company_name}): {e}")
            return None

    def get_business_report(self, corp_code):
        """Retrieve business report information"""
        url = f"{self.base_url}/list.json"
        current_year = datetime.now().year
        
        reports_by_type = {}
        report_types = {
            'A': 'ì‚¬ì—…ë³´ê³ ì„œ',
            'F': 'ë°˜ê¸°ë³´ê³ ì„œ',
            'Q': 'ë¶„ê¸°ë³´ê³ ì„œ'
        }
        
        for report_type in report_types.keys():
            params = {
                "crtfc_key": self.api_key,
                "corp_code": corp_code,
                "bgn_de": f"{current_year-1}0101",
                "end_de": datetime.now().strftime("%Y%m%d"),
                "pblntf_ty": report_type,
                "last_reprt_at": "Y"
            }
            
            try:
                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()
                
                if data.get("status") == "000" and data.get("list"):
                    reports_by_type[report_type] = data.get("list")[0]
                    print(f"Found {report_types[report_type]}: {data['list'][0].get('rpt_nm')}")
            except Exception as e:
                print(f"Error retrieving {report_types[report_type]}: {e}")
        
        # Priority: Annual > Semi-annual > Quarterly
        if 'A' in reports_by_type:
            return reports_by_type['A']
        elif 'F' in reports_by_type:
            return reports_by_type['F']
        elif 'Q' in reports_by_type:
            return reports_by_type['Q']
            
        return None

    def download_report(self, rcept_no):
        """Download business report document"""
        url = f"{self.base_url}/document.xml"
        params = {
            "crtfc_key": self.api_key,
            "rcept_no": rcept_no
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            with zipfile.ZipFile(BytesIO(response.content)) as z:
                file_list = z.namelist()
                if not file_list:
                    raise ValueError("ZIP file is empty")
                
                xml_content = z.read(file_list[0]).decode('utf-8', errors='ignore')
                
                # Debug: Save XML file
                debug_dir = "debug"
                os.makedirs(debug_dir, exist_ok=True)
                debug_file = os.path.join(debug_dir, f"report_{rcept_no}.xml")
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(xml_content)
                print(f"- XML file saved: {debug_file}")
                
                return xml_content
                
        except Exception as e:
            print(f"Error downloading report (rcept_no: {rcept_no}): {e}")
            return None

    def extract_section(self, xml_content):
        """Extract contents from major sections I and II"""
        try:
            print("Starting section extraction...")
            soup = BeautifulSoup(xml_content, 'xml')
            
            if not soup.find():
                print("Failed to parse XML content")
                return None
            
            section_patterns = [
                # ì£¼ìš” ì„¹ì…˜
                r"I\.?\s*íšŒì‚¬ì˜\s*ê°œìš”",
                r"II\.?\s*ì‚¬ì—…ì˜\s*ë‚´ìš©",
                r"III\.?\s*ì¬ë¬´ì—\s*ê´€í•œ\s*ì‚¬í•­",
                r"IV\.?\s*ì´ì‚¬ì˜\s*ê²½ì˜ì§„ë‹¨\s*ë°\s*ë¶„ì„ì˜ê²¬",
                r"V\.?\s*ì£¼ì£¼ì—\s*ê´€í•œ\s*ì‚¬í•­",
                
                # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
                r"1\.?\s*íšŒì‚¬ì˜\s*ê°œìš”",
                r"2\.?\s*ì‚¬ì—…ì˜\s*ë‚´ìš©",
                r"3\.?\s*ì¬ë¬´ì—\s*ê´€í•œ\s*ì‚¬í•­",
                r"4\.?\s*ì´ì‚¬ì˜\s*ê²½ì˜ì§„ë‹¨",
                r"5\.?\s*ì£¼ì£¼ì—\s*ê´€í•œ\s*ì‚¬í•­",
                
                # í•˜ìœ„ ì„¹ì…˜
                r"ê°€\.\s*ì—…ê³„ì˜\s*í˜„í™©",
                r"ë‚˜\.\s*íšŒì‚¬ì˜\s*í˜„í™©",
                r"ë‹¤\.\s*ì‚¬ì—…ë¶€ë¬¸ë³„\s*í˜„í™©",
                r"ë¼\.\s*ì‹ ê·œì‚¬ì—…\s*ë“±ì˜\s*ë‚´ìš©",
                r"ë§ˆ\.\s*ì¡°ì§ë„",
                r"ë°”\.\s*ì¬ë¬´ìƒíƒœ\s*ë°\s*ì˜ì—…ì‹¤ì ",
                
                # ì£¼ìš” í•˜ìœ„ í‚¤ì›Œë“œ
                r"ì‚¬ì—…ì˜\s*ë‚´ìš©",
                r"ì£¼ìš”\s*ì œí’ˆ",
                r"ë§¤ì¶œ\s*í˜„í™©",
                r"ì‹œì¥\s*ì ìœ ìœ¨",
                r"ì‹ ê·œ\s*ì‚¬ì—…",
                r"ì£¼ìš”\s*ê³ ê°",
                r"ìƒì‚°\s*ëŠ¥ë ¥",
                r"ì—°êµ¬ê°œë°œ",
                r"ì‹œì¥\s*ì „ë§"
            ]
            
            contents = []
            for title in soup.find_all(['TITLE', 'SUBTITLE']):
                title_text = title.get_text(strip=True)
                print(f"Found title: {title_text}")
                
                for pattern in section_patterns:
                    if re.search(pattern, title_text, re.IGNORECASE):
                        print(f"Matched section: {title_text}")
                        current = title.find_next()
                        while current and current.name != 'TITLE':
                            if current.name in ['P', 'TABLE', 'SPAN', 'SUBTITLE']:
                                text = current.get_text(strip=True)
                                if text and len(text) > 5:
                                    contents.append(text)
                            current = current.find_next()
            
            if not contents:
                print("No content found in major sections")
                return None
            
            # Remove duplicates while preserving order
            cleaned_contents = []
            seen = set()
            for content in contents:
                normalized_content = ' '.join(content.split())
                if normalized_content not in seen:
                    cleaned_contents.append(content)
                    seen.add(normalized_content)
            
            final_text = "\n".join(cleaned_contents)
            print(f"Final extracted content length: {len(final_text)}")
            
            return final_text
            
        except Exception as e:
            print(f"Error in extract_section: {e}")
            return None

    def chunk_text(self, text, max_tokens=3000):
        """Split text into chunks of approximately max_tokens"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0

        for word in words:
            word_token_estimate = len(word) * 1.3  # í† í° ê°œìˆ˜ ì¶”ì •

            if current_length + word_token_estimate > max_tokens:
                if current_chunk:
                    chunks.append(" ".join(current_chunk))
                    current_chunk = [word]
                    current_length = word_token_estimate
                else:
                    chunks.append(word)  # ë‹¨ì–´ê°€ ë„ˆë¬´ ê¸¸ë‹¤ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
            else:
                current_chunk.append(word)
                current_length += word_token_estimate

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        print(f"ğŸ”¹ ì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±ë¨.")
        return chunks


    def summarize_text(self, text, company_name):
        """ì²­í¬ë³„ ìš”ì•½ì„ ìˆ˜í–‰í•œ í›„, ìµœì¢…ì ìœ¼ë¡œ ì „ì²´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ìš”ì•½"""
        if not text:
            return "No content to summarize"

        chunks = self.chunk_text(text, max_tokens=3000)
        partial_summaries = []

        # 1ï¸âƒ£ ê° ì²­í¬ë³„ ë¶€ë¶„ ìš”ì•½ ìˆ˜í–‰
        for i, chunk in enumerate(chunks):
            try:
                response = requests.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.openai_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "gpt-3.5-turbo-16k",
                        "messages": [
                            {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸°ì—…ì˜ êµ¬ì²´ì ì¸ ì‚¬ì—… ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” ê²½ì˜ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì‚¬ì—…/ì œí’ˆ/ì„œë¹„ìŠ¤ìŠ¤ ë‚´ìš©ì„ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œì™€ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ì„œ ìš”ì•½í•˜ì„¸ìš”."},
                            {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{chunk}"}
                        ],
                        "temperature": 0.3,
                        "max_tokens": 5000
                    }
                )
                response.raise_for_status()
                partial_summaries.append(response.json()["choices"][0]["message"]["content"].strip())
            
            except Exception as e:
                print(f"Summarization error: {e}")
                partial_summaries.append("ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        # 2ï¸âƒ£ ë¶€ë¶„ ìš”ì•½ëœ ë‚´ìš©ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
        combined_summary = "\n".join(partial_summaries)

        # 3ï¸âƒ£ ì „ì²´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìˆ˜í–‰
        try:
            final_response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openai_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-3.5-turbo-16k",
                    "messages": [
                    {"role": "system", "content": """ë‹¹ì‹ ì€ ê¸°ì—…ì˜ êµ¬ì²´ì ì¸ ì‚¬ì—… ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” ê²½ì˜ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ê¸°ì—…ì˜ ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ì‚¬ì—…ê³¼ ì œí’ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ê°ê°ì˜ í•µì‹¬ íŠ¹ì„±ì´ë‚˜ ìš©ë„ë¥¼ ì •í™•íˆ ì„œìˆ í•˜ì‹­ì‹œì˜¤.

                    ë˜í•œ, ë³´ê³ ì„œì— í¬í•¨ëœ ì •ëŸ‰ì  ë°ì´í„°(í‰ê·  íŒë§¤ ê°€ê²©, ì¶œí•˜ëŸ‰, ë§¤ì¶œ ë¹„ì¤‘ ë“±)ê°€ ìˆì„ ê²½ìš° ì´ë¥¼ ì¤‘ìš”í•œ ì •ë³´ë¡œ ë°˜ì˜í•˜ì‹­ì‹œì˜¤. í•˜ì§€ë§Œ, ì—†ëŠ” ê²½ìš° ì„ì˜ë¡œ ìƒì„±í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
                    
                    * ì£¼ì˜ì‚¬í•­:
                    1. ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ë‚´ìš©ë§Œ í¬í•¨
                    2. êµ¬ì²´ì ì¸ ì œí’ˆ/ì„œë¹„ìŠ¤/ê¸°ìˆ ëª… í•„ìˆ˜ í¬í•¨
                    3. ì •ëŸ‰ì  ë°ì´í„°ê°€ ë³´ê³ ì„œì— ìˆìœ¼ë©´ í¬í•¨í•˜ë˜, ì—†ëŠ” ê²½ìš° ìƒì„±í•˜ì§€ ì•ŠìŒ
                    4. ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜ì ì¸ ì„¤ëª…ë³´ë‹¤ëŠ” ê¸°ì—…ì˜ êµ¬ì²´ì ì¸ ìƒí™©ì„ ë°˜ì˜
                    5. ê° ë¬¸ì¥ì€ ë‚´ìš©ì´ ë¶„ë¦¬ëœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê°œë³„ í•­ëª©(-)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„± 

                    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:

                    0. ì „ì²´ ê¸°ì—… ë‚´ìš© ìš”ì•½
                    - í•´ë‹¹ ê¸°ì—…ì˜ í•µì‹¬ ì‚¬ì—…, ì œí’ˆ, ì‹œì¥ ë™í–¥ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½.
                    - ë³´ê³ ì„œì—ì„œ ì œê³µí•˜ëŠ” ì •ëŸ‰ì  ë°ì´í„°(í‰ê·  íŒë§¤ ê°€ê²©, ì¶œí•˜ëŸ‰, ë§¤ì¶œ ë¹„ì¤‘ ë“±)ê°€ ìˆìœ¼ë©´ í¬í•¨í•˜ì—¬ íŠ¸ë Œë“œ ë¶„ì„.

                    1. ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤
                    - ì œí’ˆëª…ê³¼ ì„œë¹„ìŠ¤ëª…ì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•˜ê³ , í•´ë‹¹ ì œí’ˆì˜ í•µì‹¬ ê¸°ëŠ¥ ë° ìš©ë„ë¥¼ ì„¤ëª….
                    - ì‹œì¥ ë°˜ì‘, í‰ê·  íŒë§¤ ê°€ê²© ë³€ë™, ë§¤ì¶œ ê¸°ì—¬ë„ ë³€í™” ë“±ì˜ ì •ëŸ‰ì  ì •ë³´ê°€ ìˆìœ¼ë©´ í¬í•¨.

                    2. ì£¼ìš” ê¸°ìˆ  ë° ì¸í”„ë¼
                    - ì‚¬ìš©ëœ ê¸°ìˆ ì˜ í•µì‹¬ íŠ¹ì„±ê³¼ í•´ë‹¹ ê¸°ìˆ ì´ ì ìš©ëœ ì œí’ˆ/ì„œë¹„ìŠ¤ë¥¼ ì„¤ëª….
                    - ìƒì‚°ëŠ¥ë ¥, ë¹„ìš© ì ˆê° íš¨ê³¼ ë“± ê´€ë ¨ëœ ìˆ˜ì¹˜ì  ë³€í™”ê°€ ìˆìœ¼ë©´ í¬í•¨.

                    3. í•µì‹¬ ì‚¬ì—… ì˜ì—­
                    - í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì£¼ìš” ì‚¬ì—… í™œë™ì„ ì„¤ëª…í•˜ê³ , í–¥í›„ ì„±ì¥ ì „ëµì´ ì•„ë‹ˆë¼ í˜„í™©ì— ì´ˆì ì„ ë§ì¶¤.
                    - ì‹œì¥ ë°˜ì‘, ì„±ì¥ë¥ , ë§¤ì¶œ ê¸°ì—¬ë„ ë“±ì˜ ì •ëŸ‰ì  ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì´ë¥¼ ê°•ì¡° (% ìˆ˜ì¹˜ê°€ ìˆë‹¤ë©´ ì¤‘ìš” ì •ë³´ë¡œ ê°„ì£¼).

                    """},

                    {"role": "user", "content": f"ë‹¤ìŒì€ {company_name}ì˜ ì‚¬ì—… ê´€ë ¨ ì£¼ìš” ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ„ì˜ í˜•ì‹ì— ë§ì¶° ì „ì²´ì ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬ ìš”ì•½í•˜ì‹­ì‹œì˜¤:\n\n{combined_summary}"}
                ],
                    "temperature": 0.3,
                    "max_tokens": 4000
                }
            )
            final_response.raise_for_status()
            return final_response.json()["choices"][0]["message"]["content"].strip()

        except Exception as e:
            print(f"Final summarization error: {e}")
            return "ìµœì¢… ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # def summarize_text(self, text, company_name):
    #     """Summarize text using GPT-3.5-Turbo with chunking"""
    #     if not text:
    #         return "No content to summarize"
        
    #     chunks = self.chunk_text(text, max_tokens=3000)
    #     summaries = []
        
    #     for chunk in chunks:
    #         try:
    #             response = requests.post(
    #                 "https://api.openai.com/v1/chat/completions",
    #                 headers={
    #                     "Authorization": f"Bearer {self.openai_api_key}",
    #                     "Content-Type": "application/json"
    #                 },
    #                 json={
    #                     "model": "gpt-3.5-turbo-16k",
    #                     "messages": [
    #                         {"role": "system", "content": """ê¸°ì—…ì˜ ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ì‚¬ì—…ê³¼ ì œí’ˆì„ êµ¬ì²´ì  í‚¤ì›Œë“œë¡œ ì¶”ì¶œí•˜ê³ , ê°ê°ì˜ ì£¼ìš” íŠ¹ì„±ì´ë‚˜ ìš©ë„ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

    #                         âŒ í”¼í•´ì•¼ í•  í‘œí˜„:
    #                         - "~ë¥¼ ê°•í™”í•  ì˜ˆì •"
    #                         - "~ì‹œì¥ ì§„ì¶œ ê³„íš"
    #                         - "~ì „ëµì„ ì¶”ì§„"
    #                         - "~ê²½ìŸë ¥ í–¥ìƒ"

    #                         âœ… ë°”ëŒì§í•œ í‘œí˜„:
    #                         - "'ì œí’ˆëª…A': ê³ ì„±ëŠ¥ í”„ë¦¬ë¯¸ì—„ ìŠ¤ë§ˆíŠ¸í°, í´ë”ë¸” ë””ìŠ¤í”Œë ˆì´ íƒ‘ì¬"
    #                         - "'ì„œë¹„ìŠ¤ëª…B': AI ê¸°ë°˜ ë²ˆì—­ ì„œë¹„ìŠ¤, 109ê°œ ì–¸ì–´ ì§€ì›"
    #                         - "'ê¸°ìˆ ëª…C': 5ë‚˜ë…¸ ë°˜ë„ì²´ ì œì¡° ê³µì •, ëª¨ë°”ì¼ AP ìƒì‚°ì— ì ìš©"

    #                         ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:

    #                         0. ì „ì²´ ê¸°ì—… ë‚´ìš© ìš”ì•½
    #                         - ì£¼ìš” ì‚¬ì—…/ì œí’ˆ/ì„œë¹„ìŠ¤ ë‚´ìš©ì„ ìš”ì•½
                            
    #                         1. ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤
    #                         - ì£¼ìš” íŠ¹ì„±/ìš©ë„ ì„¤ëª…, í•µì‹¬ ê¸°ëŠ¥/íŠ¹ì§• ì„¤ëª…, ì‹œì¥ ë°˜ì‘ ì„¤ëª… 

    #                         2. ì£¼ìš” ê¸°ìˆ  ë° ì¸í”„ë¼
    #                         - ì ìš© ì œí’ˆ/ìš©ë„ ì„¤ëª…, ìƒì‚°í’ˆëª©/ëŠ¥ë ¥ ì„¤ëª…, ì‹œì¥ ë°˜ì‘ ì„¤ëª… 

    #                         3. í•µì‹¬ ì‚¬ì—… ì˜ì—­
    #                         - ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ë‚´ìš© ì„¤ëª…, í˜„ì¬ ì§„í–‰ ë‹¨ê³„/ê·œëª¨ ì„¤ëª…

    #                         * ì£¼ì˜ì‚¬í•­:
    #                         1. ì‹¤ì œ ì§„í–‰ ì¤‘ì¸ ë‚´ìš©ë§Œ í¬í•¨ (ê³„íšì´ë‚˜ ì „ëµ ì œì™¸)
    #                         2. êµ¬ì²´ì ì¸ ì œí’ˆ/ì„œë¹„ìŠ¤/ê¸°ìˆ ëª… í•„ìˆ˜ í¬í•¨
    #                         3. ê° í•­ëª©ì˜ ì‹¤ì œ íŠ¹ì„±ì´ë‚˜ ìš©ë„ë¥¼ êµ¬ì²´ì ìœ¼ë¡œë¡œ ì„¤ëª…"""},
    #                         {"role": "user", "content": f"ë‹¤ìŒ {company_name}ì˜ ì‚¬ì—…ë³´ê³ ì„œì—ì„œ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ êµ¬ì²´ì ì¸ ì‚¬ì—…ê³¼ ì œí’ˆì„ ì¶”ì¶œí•˜ê³ , ê°ê°ì˜ ì£¼ìš” íŠ¹ì„±ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:\n\n{chunk}"}
    #                     ],
    #                     "temperature": 0.3,
    #                     "max_tokens": 3000
    #                 }
    #             )
    #             response.raise_for_status()
    #             summaries.append(response.json()["choices"][0]["message"]["content"].strip())
    #         except Exception as e:
    #             print(f"Summarization error: {e}")
    #             summaries.append("ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
    #     return "\n".join(summaries) if summaries else "ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."



    def process_company(self, company_name, stock_code):
        """Process a single company"""
        try:
            print(f"\nProcessing {company_name}...")
            corp_code = self.corp_codes.get(stock_code)
            if not corp_code:
                print(f"Corporate code not found for {company_name}")
                return None
            
            company_info = self.get_company_info(company_name, stock_code)
            if not company_info:
                print(f"Company info not found for {company_name}")
                return None
            
            report = self.get_business_report(corp_code)
            if not report:
                print(f"Business report not found for {company_name}")
                return None
            
            xml_content = self.download_report(report.get("rcept_no"))
            if not xml_content:
                print(f"Failed to download report for {company_name}")
                return None
            
            report_content = self.extract_section(xml_content)
            if not report_content:
                print(f"Failed to extract sections for {company_name}")
                return None
            
            summary = self.summarize_text(report_content, company_name)
            
            if "ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤." in summary:
                print("Retrying with chunked summarization...")
                summary = self.summarize_text(report_content, company_name)
            
            return {
                "company_name": company_name,
                "company_info": company_info,
                "business_overview": report_content,
                "business_overview_summary": summary
            }
        except Exception as e:
            print(f"Error processing {company_name}: {e}")
            return None


    def upload_to_elasticsearch(self, company_data):
        """Upload data to Elasticsearch with update functionality"""
        print("\nStarting Elasticsearch upload process...")
        headers = {"Content-Type": "application/json"}
        
        company_name = company_data.get("company_name")
        business_overview_summary = company_data.get("business_overview_summary", "")
        original_content = company_data.get("business_overview", "")
        company_info = company_data.get("company_info", {})
        
        if not business_overview_summary:
            print(f"Warning: No summary data for {company_name}, skipping.")
            return
            
        print(f"Preparing data for {company_name}")
        doc = {
            "company_name": company_name,
            "business_overview_summary": business_overview_summary,
            "business_overview_original": original_content,
            "company_info": company_info,
            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        try:
            # Search for existing document
            search_response = requests.post(
                f"{self.es_url}/{self.index_name}/_search",
                json={
                    "query": {
                        "match": {
                            "company_name.keyword": company_name
                        }
                    }
                },
                headers=headers,
                timeout=30
            )
            
            search_result = search_response.json()
            hits = search_result.get('hits', {}).get('hits', [])
            
            if hits:
                # Update existing document
                doc_id = hits[0]['_id']
                print(f"Found existing document with ID: {doc_id}")
                response = requests.post(
                    f"{self.es_url}/{self.index_name}/_update/{doc_id}",
                    json={"doc": doc},
                    headers=headers,
                    timeout=30
                )
            else:
                # Create new document
                print("No existing document found, creating new one")
                response = requests.post(
                    f"{self.es_url}/{self.index_name}/_doc",
                    json=doc,
                    headers=headers,
                    timeout=30
                )
            
            if response.status_code in [200, 201]:
                print(f"Successfully {'updated' if hits else 'created'} data for {company_name}")
                print(f"Response: {response.json()}")
            else:
                print(f"Failed to {'update' if hits else 'create'} data for {company_name}")
                print(f"Status code: {response.status_code}")
                print(f"Response: {response.text}")
                
        except requests.exceptions.Timeout:
            print(f"Timeout occurred while uploading data for {company_name}")
        except requests.exceptions.ConnectionError:
            print(f"Connection error occurred while uploading data for {company_name}")
        except Exception as e:
            print(f"Error uploading to Elasticsearch: {e}")
        
        print("Upload process completed")

    def save_individual_result(self, company_data, output_dir="output"):
        """Save individual company result"""
        os.makedirs(output_dir, exist_ok=True)
        company_name = company_data["company_name"]
        file_path = os.path.join(output_dir, f"{company_name}_report.json")
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(company_data, f, ensure_ascii=False, indent=2)
        print(f"Saved result for {company_name} to: {file_path}")

    def save_results(self, results, output_dir="output"):
        """Save results to JSON files"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Save combined results
        combined_path = os.path.join(output_dir, "company_reports.json")
        with open(combined_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nSaved combined results to: {combined_path}")
        
        # Save individual company results
        for company_data in results:
            self.save_individual_result(company_data)

    def run(self):
        """Main execution method"""
        results = []
        
        try:
            print("Starting business analysis system...")
            self.download_corp_codes()
            
            for company_name, stock_code in self.companies.items():
                try:
                    company_data = self.process_company(company_name, stock_code)
                    if company_data:
                        # Save data immediately after processing each company
                        self.upload_to_elasticsearch(company_data)
                        self.save_individual_result(company_data)
                        results.append(company_data)
                        
                except Exception as e:
                    print(f"Error processing {company_name}: {e}")
                    continue
            
            # Save final combined results
            self.save_results(results)
            print("\nProcessing completed successfully")
            
        except Exception as e:
            print(f"Fatal error in main execution: {e}")
            raise

def main():
    try:
        system = BusinessAnalysisSystem()
        system.run()
    except Exception as e:
        print(f"Program terminated with error: {e}")

if __name__ == "__main__":
    main()